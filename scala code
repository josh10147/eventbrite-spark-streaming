import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.sql.SaveMode

object eventbrite_stream {

  def main(args: Array[String]): Unit = {

    // Spark Config
    val conf = new SparkConf()
      .setAppName("eventbrite-streaming")
      .setMaster("local[*]")

    val ssc = new StreamingContext(conf, Seconds(15))

    // Kafka Config
    val topics = List("eventbrite").toSet

    val kafkaParams = Map(
      "bootstrap.servers" -> "sandbox.hortonworks.com:6667",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "consumer-group", // Your consumer group
      "auto.offset.reset" -> "earliest")

    // Retrieve data from Kafka into Dstream Object
    val kafka_stream_Dstream = KafkaUtils.createDirectStream[String,String](
      ssc,
      PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](topics, kafkaParams))

    // Transformation - converting all chars to lower cases

    val lower_Dstream = kafka_stream_Dstream.map(record => record.value().toString.toLowerCase)
    lower_Dstream.foreachRDD(rddRaw => {
      val spark = SparkSession.builder.config(rddRaw.sparkContext.getConf).getOrCreate()
      val cols = List("events.name.text","events.venue.address.city", "events.venue.address.country",
        "events.start.utc", "events.is_free","events.currency", "events.format.name")

      val df = spark.read.json(rddRaw).select(cols.head, cols.tail: _*)

      lower_Dstream.foreachRDD(eachRdd => {
      eachRdd.saveAsTextFile("hdfs://sandbox.hortonworks.com:8020/user/maria_dev/output")
    
    })
    ssc.start()
    ssc.awaitTermination()
  }
}
